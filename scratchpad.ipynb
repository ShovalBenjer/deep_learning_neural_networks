{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/deep_learning_neural_networks/blob/main/scratchpad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Language Model Integration with Forward/Reverse Training  \n",
        "   \n",
        "This notebook implements the assignment requirements using the repository code in  \n",
        "`util.py` and `words.py` (which have been modified as described).  \n",
        "   \n",
        "**Tasks completed:**  \n",
        "1. The dataset is split into train (80%), validation (10%), and test (10%) sets.  \n",
        "2. A perplexity metric is computed on each split after training.  \n",
        "3. We support running the LSTM both in the natural (forward) order and in reverse order (using Keras’s `go_backwards` flag).  \n",
        "4. We train 4 LSTM models: one–layer vs. two–layer, and for each a forward and a reverse version.  \n",
        "5. A function is provided that computes the probability of a given sentence from a trained model.  \n",
        "6. A sentence of length 7 starting with \"love I\" is generated at temperatures 0.1, 1, and 10.  \n",
        "7. An interactive UI function allows entering a seed word to obtain the next predicted word.  \n",
        "8. For each model, perplexity is recorded for train, validation, and test sets (12 results total).  \n",
        "9. The probability is computed for the generated sentence and also for the sentence \"love i cupcakes\".\n",
        "\n",
        "All training progress is logged via TensorBoardX.\n"
      ],
      "metadata": {
        "id": "BflmPIL2Ve_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n"
      ],
      "metadata": {
        "id": "neeYvGg5VwEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX\n",
        "!pip install nltk\n",
        "!pip install keras-preprocessing\n",
        "!pip install tensorflow-gpu\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install keras-preprocessing-gpu\n",
        "# %tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "!pip install tensorboardX\n",
        "# !pip install language_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnJOPngRVzAj",
        "outputId": "bcbbc2f7-621a-412b-ff34-484392c46fef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (4.25.6)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from keras-preprocessing) (1.26.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from keras-preprocessing) (1.17.0)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-preprocessing\n",
            "Successfully installed keras-preprocessing-1.1.2\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement keras-preprocessing-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for keras-preprocessing-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (4.25.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A-JXVX0Z5oV",
        "outputId": "09438b91-13fa-4f57-be25-5c0b7b512f31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPU Available: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone the Repository  \n",
        "   \n",
        "We now clone the repository from GitHub and check out the correct branch.\n"
      ],
      "metadata": {
        "id": "5ChsewebV1Oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShovalBenjer/deep_learning_neural_networks.git\n",
        "%cd deep_learning_neural_networks\n",
        "!git checkout language-models-integration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "323BLKz4V3QQ",
        "outputId": "19a30f00-321e-41c3-cca2-7897e89e507b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep_learning_neural_networks'...\n",
            "remote: Enumerating objects: 215, done.\u001b[K\n",
            "remote: Counting objects: 100% (215/215), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 215 (delta 113), reused 103 (delta 55), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (215/215), 21.34 MiB | 12.10 MiB/s, done.\n",
            "Resolving deltas: 100% (113/113), done.\n",
            "/content/deep_learning_neural_networks\n",
            "Branch 'language-models-integration' set up to track remote branch 'language-models-integration' from 'origin'.\n",
            "Switched to a new branch 'language-models-integration'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Setup  \n",
        "   \n",
        "We import necessary modules from Keras, tensorboardX, and our own `util.py` and `words.py`.\n"
      ],
      "metadata": {
        "id": "WAXw5A7pV5Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, numpy as np, keras, keras.backend as K, nltk, tensorflow as tf\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Input, Dense\n",
        "from keras.models import Model\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "nltk.download('punkt')\n",
        "from keras.datasets import imdb\n",
        "import\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tqdm import tqdm\n",
        "from argparse import ArgumentParser\n",
        "from tensorboardX import SummaryWriter\n",
        "CHECK = 5\n",
        "\n",
        "# Import our custom modules (make sure util.py and words.py are in the repo)\n",
        "import util\n",
        "import words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "C18Z59xYV56l",
        "outputId": "88f862ec-e781-48fd-ab17-d12da4f69cea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-3961d4e9aa44>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-3961d4e9aa44>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    import\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions  \n",
        "   \n",
        "The following helper functions are defined:\n",
        "   \n",
        "- **split_data:** Splits a numpy array of padded sentences into train/val/test sets (80–10–10).  \n",
        "- **compute_perplexity:** Computes average loss on a dataset and returns perplexity = exp(loss).  \n",
        "- **sentence_probability:** Computes the probability (and log probability) of a sentence given a trained model.  \n",
        "- **interactive_next_word:** A simple UI function that takes a seed sequence and prints the next predicted word.\n"
      ],
      "metadata": {
        "id": "x82C2IIrWB9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Splitting the Data  \n",
        "   \n",
        "We load the dataset using `util.load_words` from the file `datasets/wikisimple.txt` and then pad the sentences.  \n",
        "Next, we concatenate all batches into one array and split it (80% train, 10% validation, 10% test).\n"
      ],
      "metadata": {
        "id": "ModHvdIZWIad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Split a numpy array (of shape [num_sentences, seq_length]) into train, validation, and test arrays.\n",
        "    \"\"\"\n",
        "    indices = np.arange(data.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    n = len(indices)\n",
        "    train_end = int(train_ratio * n)\n",
        "    val_end = int((train_ratio + val_ratio) * n)\n",
        "    train_data = data[indices[:train_end]]\n",
        "    val_data = data[indices[train_end:val_end]]\n",
        "    test_data = data[indices[val_end:]]\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def compute_perplexity(model, data, batch_size=128):\n",
        "    \"\"\"\n",
        "    Compute perplexity on a dataset.\n",
        "\n",
        "    The loss is calculated using sparse categorical crossentropy.\n",
        "    Perplexity is computed as exp(average_loss).\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    for i in range(0, data.shape[0], batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        n = batch.shape[0]\n",
        "        batch_in = np.concatenate([np.ones((n, 1), dtype='int32'), batch], axis=1)\n",
        "        batch_out = np.concatenate([batch, np.zeros((n, 1), dtype='int32')], axis=1)\n",
        "        loss = model.test_on_batch(batch_in, batch_out[:,:,None])\n",
        "        losses.append(loss)\n",
        "    avg_loss = np.mean(losses)\n",
        "    return np.exp(avg_loss)\n",
        "\n",
        "def sentence_probability(sentence, model, w2i, i2w):\n",
        "    \"\"\"\n",
        "    Compute the probability and log-probability of a given sentence.\n",
        "\n",
        "    The sentence is tokenized by spaces; unknown words are replaced by <UNK>.\n",
        "    The model is assumed to predict the next word given the previous tokens.\n",
        "    \"\"\"\n",
        "    words_in = sentence.strip().split()\n",
        "    # Use <UNK> index if word not found\n",
        "    unk = w2i.get('<UNK>', 2)\n",
        "    token_ids = [w2i.get(word.lower(), unk) for word in words_in]\n",
        "    # Prepend <START> token (assumed index 1)\n",
        "    token_ids = [1] + token_ids\n",
        "    token_ids = np.array(token_ids)[None, :]  # shape (1, seq_len)\n",
        "    logits = model.predict(token_ids)\n",
        "    # Apply softmax to obtain probabilities at each time step\n",
        "    probs = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n",
        "    p = 1.0\n",
        "    logp = 0.0\n",
        "    # For t=1 to end, probability assigned to token at position t given context t-1\n",
        "    for t in range(1, token_ids.shape[1]):\n",
        "        prob = probs[0, t-1, token_ids[0, t]]\n",
        "        p *= prob\n",
        "        logp += np.log(prob + 1e-10)\n",
        "    return p, logp\n",
        "\n",
        "def interactive_next_word(model, seed_seq, w2i, i2w):\n",
        "    \"\"\"\n",
        "    Given a seed sequence (list of word indices), generate one additional word using the model.\n",
        "    \"\"\"\n",
        "    seed_seq = np.array(seed_seq)\n",
        "    seed_seq = np.insert(seed_seq, 0, 1)  # Prepend <START> token (index 1)\n",
        "    gen = words.generate_seq(model, seed_seq, size=seed_seq.shape[0]+1, temperature=1.0)\n",
        "    next_idx = gen[-1]\n",
        "    seed_words = ' '.join(i2w[str(idx)] for idx in seed_seq[1:])\n",
        "    print(\"Seed: \", seed_words)\n",
        "    print(\"Next predicted word: \", i2w[str(next_idx)])\n"
      ],
      "metadata": {
        "id": "GigfQphuWGOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building and Training Functions  \n",
        "   \n",
        "We define two functions:  \n",
        "   \n",
        "- **build_model:** Constructs the Keras model. It uses the parameter `reverse` to decide whether the LSTM layers operate in reverse order (by setting `go_backwards=True`). An extra LSTM layer is added if requested.  \n",
        "- **train_model:** Trains the model on the training set in a custom loop, logs loss via TensorBoardX, and after training computes perplexity on the train, validation, and test sets. It also demonstrates sample generation.\n"
      ],
      "metadata": {
        "id": "mTMJc5u4WYTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(numwords, lstm_capacity, extra, reverse):\n",
        "    \"\"\"\n",
        "    Build and return a Keras Model for language modeling.\n",
        "\n",
        "    :param numwords: Size of the vocabulary.\n",
        "    :param lstm_capacity: The dimensionality of the LSTM hidden state.\n",
        "    :param extra: Number of extra LSTM layers (None means only one layer).\n",
        "    :param reverse: Boolean flag; if True, LSTM layers use go_backwards=True.\n",
        "    :return: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    inp = Input(shape=(None,))\n",
        "    embed = Embedding(numwords, lstm_capacity)\n",
        "    x = embed(inp)\n",
        "    # First LSTM layer with reverse flag\n",
        "    x = LSTM(lstm_capacity, return_sequences=True, go_backwards=reverse)(x)\n",
        "    # Extra LSTM layers if any\n",
        "    if extra is not None:\n",
        "        for _ in range(extra):\n",
        "            x = LSTM(lstm_capacity, return_sequences=True, go_backwards=reverse)(x)\n",
        "    dense = Dense(numwords, activation='linear')\n",
        "    out = TimeDistributed(dense)(x)\n",
        "    model = Model(inp, out)\n",
        "    return model\n",
        "\n",
        "def train_model(options, train_data, val_data, test_data, w2i, i2w):\n",
        "    \"\"\"\n",
        "    Build and train a language model using globally padded data.\n",
        "    Expects train_data, val_data, and test_data to be numpy arrays of shape [num_sentences, seq_length].\n",
        "\n",
        "    The training loop:\n",
        "      - Shuffles the training data each epoch.\n",
        "      - Iterates over mini-batches.\n",
        "      - Logs training loss via TensorBoardX.\n",
        "      - After training, computes perplexity on train, validation, and test sets.\n",
        "      - Generates sample sentences and computes sentence probabilities.\n",
        "\n",
        "    :param options: An options object with training hyperparameters.\n",
        "    :param train_data, val_data, test_data: Numpy arrays (globally padded) of shape [N, L].\n",
        "    :param w2i, i2w: Word-to-index and index-to-word dictionaries.\n",
        "    :return: The trained Keras model.\n",
        "    \"\"\"\n",
        "    writer = SummaryWriter(log_dir=options.tb_dir)\n",
        "    np.random.seed(options.seed)\n",
        "\n",
        "    numwords = len(i2w)\n",
        "    model = build_model(numwords, options.lstm_capacity, options.extra, options.reverse)\n",
        "    opt = keras.optimizers.Adam(learning_rate=options.lr)\n",
        "    model.compile(opt, sparse_loss)\n",
        "    model.summary()\n",
        "\n",
        "    num_train = train_data.shape[0]\n",
        "    instances_seen = 0\n",
        "    for epoch in range(options.epochs):\n",
        "        indices = np.arange(num_train)\n",
        "        np.random.shuffle(indices)\n",
        "        train_data = train_data[indices]\n",
        "\n",
        "        for i in tqdm(range(0, num_train, options.batch)):\n",
        "            batch = train_data[i:i+options.batch]\n",
        "            n, l = batch.shape\n",
        "            # Prepend start symbol (index 1) and append pad symbol (index 0)\n",
        "            batch_in = np.concatenate([np.ones((n, 1), dtype='int32'), batch], axis=1)\n",
        "            batch_out = np.concatenate([batch, np.zeros((n, 1), dtype='int32')], axis=1)\n",
        "\n",
        "            loss = model.train_on_batch(batch_in, batch_out[:, :, None])\n",
        "            instances_seen += n\n",
        "            writer.add_scalar('lm/train_batch_loss', float(loss), instances_seen)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} complete\")\n",
        "\n",
        "        # Generate some sample sentences at various temperatures\n",
        "        for temp in [0.0, 0.9, 1.0, 1.1, 1.2]:\n",
        "            print(\"### TEMP\", temp)\n",
        "            for _ in range(CHECK):\n",
        "                # 'b' is a 1D array if randomly chosen from train_data\n",
        "                b = random.choice(train_data)\n",
        "                # Fix: slice a 1D array, not [0, :...]\n",
        "                seed = b[:min(20, b.shape[0])]\n",
        "                seed = np.insert(seed, 0, 1)  # Prepend <START>\n",
        "\n",
        "                gen = words.generate_seq(model, seed, 60, temperature=temp)\n",
        "                def decode(seq):\n",
        "                    return ' '.join(i2w[i] if i < len(i2w) else '<UNK>' for i in seq)\n",
        "\n",
        "                print('*** [', decode(seed), '] ', decode(gen[len(seed):]))\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    # Compute perplexity on the three sets\n",
        "    ppl_train = compute_perplexity(model, train_data, options.batch)\n",
        "    ppl_val = compute_perplexity(model, val_data, options.batch)\n",
        "    ppl_test = compute_perplexity(model, test_data, options.batch)\n",
        "\n",
        "    print(\"Perplexity (Train): {:.2f}\".format(ppl_train))\n",
        "    print(\"Perplexity (Validation): {:.2f}\".format(ppl_val))\n",
        "    print(\"Perplexity (Test): {:.2f}\".format(ppl_test))\n",
        "\n",
        "    # Generate sentence of length 7 starting with \"love I\" at different temperatures\n",
        "    seed_words = \"love I\".split()\n",
        "    seed_ids = [w2i.get(word.lower(), w2i.get('<UNK>')) for word in seed_words]\n",
        "    seed_ids = np.array(seed_ids)\n",
        "    print(\"\\nSentence generation (length=7) starting with 'love I':\")\n",
        "    for temp in [0.1, 1.0, 10.0]:\n",
        "        gen = words.generate_seq(model, np.insert(seed_ids, 0, 1), size=7, temperature=temp)\n",
        "        def decode(seq):\n",
        "            return ' '.join(i2w[i] if 0 <= i < len(i2w) else '<UNK>' for i in seq)\n",
        "        print(\"Temperature {}: {}\".format(temp, decode(gen)))\n",
        "\n",
        "    # Compute probability for the generated sentence and for \"love i cupcakes\"\n",
        "    def decode(seq):\n",
        "        return ' '.join(i2w[i] if 0 <= i < len(i2w) else '<UNK>' for i in seq)\n",
        "    generated_sentence = decode(gen)\n",
        "    p, logp = sentence_probability(generated_sentence, model, w2i, i2w)\n",
        "    print(\"\\nProbability for generated sentence '{}': {:.2e} (log={:.2f})\".format(generated_sentence, p, logp))\n",
        "\n",
        "    sentence2 = \"love i cupcakes\"\n",
        "    p2, logp2 = sentence_probability(sentence2, model, w2i, i2w)\n",
        "    print(\"Probability for sentence 'love i cupcakes': {:.2e} (log={:.2f})\".format(p2, logp2))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "XuVkVCAsWZQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Options Class and Helper to Set Options  \n",
        "   \n",
        "We define a simple options container and a function to generate options for training.\n"
      ],
      "metadata": {
        "id": "tGyfF5FKWbpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Options:\n",
        "    pass\n",
        "\n",
        "def get_options(lstm_capacity=256, batch=128, epochs=10, extra=None, lr=0.001,\n",
        "                top_words=10000, limit=None, seed=42, reverse=False, tb_dir='./runs/words'):\n",
        "    opt = Options()\n",
        "    opt.lstm_capacity = lstm_capacity\n",
        "    opt.batch = batch\n",
        "    opt.epochs = epochs\n",
        "    opt.extra = extra      # extra LSTM layers (None means only one LSTM layer)\n",
        "    opt.lr = lr\n",
        "    opt.top_words = top_words\n",
        "    opt.limit = limit\n",
        "    opt.seed = seed\n",
        "    opt.reverse = reverse  # if True, LSTM layers are trained in reverse order\n",
        "    opt.tb_dir = tb_dir\n",
        "    opt.task = 'wikisimple'\n",
        "    opt.data = './data'\n",
        "    return opt\n"
      ],
      "metadata": {
        "id": "0IJHVMh1Wc-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Four Models and Recording Perplexity  \n",
        "   \n",
        "We now run experiments for four configurations:  \n",
        "1. 1-layer forward (extra=None, reverse=False)  \n",
        "2. 1-layer reverse (extra=None, reverse=True)  \n",
        "3. 2-layer forward (extra=1, reverse=False)  \n",
        "4. 2-layer reverse (extra=1, reverse=True)  \n",
        "   \n",
        "For each model we train on the training set, compute perplexity on train, validation and test sets, and record the results.\n"
      ],
      "metadata": {
        "id": "z2-Fkj9PWer7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data (x is a list of lists of integers)\n",
        "x, w2i, i2w = util.load_words(os.path.join(util.DIR, 'datasets', 'wikisimple.txt'),\n",
        "                               vocab_size=10000, limit=None)\n",
        "print(\"Number of sentences loaded (before batching):\", len(x))\n",
        "\n",
        "# Compute the global maximum sentence length\n",
        "global_max_len = max(len(sentence) for sentence in x)\n",
        "print(\"Global maximum sentence length:\", global_max_len)\n",
        "\n",
        "# Pad all sentences to the same length\n",
        "from keras_preprocessing.sequence import pad_sequences  # or from tensorflow.keras.preprocessing.sequence\n",
        "all_data = pad_sequences(x, maxlen=global_max_len, dtype='int32', padding='post', truncating='post')\n",
        "print(\"Total sentences (after global padding):\", all_data.shape[0])\n",
        "\n",
        "# Now split the padded array into train, validation, and test sets\n",
        "train_data, val_data, test_data = split_data(all_data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)\n",
        "print(\"Train: {}, Validation: {}, Test: {}\".format(train_data.shape[0], val_data.shape[0], test_data.shape[0]))\n"
      ],
      "metadata": {
        "id": "dVC_1drjbDDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model configurations\n",
        "configs = [\n",
        "    (\"1-layer_forward\", None, False),\n",
        "    (\"1-layer_reverse\", None, True),\n",
        "    (\"2-layer_forward\", 1, False),\n",
        "    (\"2-layer_reverse\", 1, True)\n",
        "]\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "trained_models = {}\n",
        "\n",
        "for name, extra, rev in configs:\n",
        "    print(\"\\n=== Training configuration: {} ===\".format(name))\n",
        "    opts = get_options(\n",
        "        lstm_capacity=64,  # Reduced LSTM size\n",
        "        batch=256,  # Larger batch size\n",
        "        epochs=5,  # Fewer epochs\n",
        "        extra=extra,\n",
        "        lr=0.01,\n",
        "        top_words=100,  # Reduced vocabulary\n",
        "        seed=42,\n",
        "        reverse=rev,\n",
        "        tb_dir='./runs/words/' + name\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(opts, train_data, val_data, test_data, w2i, i2w)\n",
        "    trained_models[name] = model  # Store trained models for later use\n",
        "\n",
        "    print(f\"✔ Training completed for: {name}\")\n"
      ],
      "metadata": {
        "id": "Urp17YkpN9il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in trained_models.items():\n",
        "    opts = get_options()  # Get options again if needed\n",
        "    ppl_train = compute_perplexity(model, train_data, opts.batch)\n",
        "    results.setdefault(name, {})[\"Train\"] = ppl_train\n",
        "    print(f\"✔ Perplexity (Train) for {name}: {ppl_train:.2f}\")"
      ],
      "metadata": {
        "id": "ZglM_6y7N9OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in trained_models.items():\n",
        "    opts = get_options()\n",
        "    ppl_test = compute_perplexity(model, test_data, opts.batch)\n",
        "    results.setdefault(name, {})[\"Test\"] = ppl_test\n",
        "    print(f\"✔ Perplexity (Test) for {name}: {ppl_test:.2f}\")"
      ],
      "metadata": {
        "id": "g9bC8OzgN9Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Summary of Perplexity Results ===\")\n",
        "for config, ppl in results.items():\n",
        "    print(config, ppl)"
      ],
      "metadata": {
        "id": "aLu30JNBN86n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "configs = [\n",
        "    (\"1-layer_forward\", None, False),\n",
        "    (\"1-layer_reverse\", None, True),\n",
        "    (\"2-layer_forward\", 1, False),\n",
        "    (\"2-layer_reverse\", 1, True)\n",
        "]\n",
        "\n",
        "for name, extra, rev in configs:\n",
        "    print(\"\\n=== Training configuration: {} ===\".format(name))\n",
        "    opts = get_options(\n",
        "        lstm_capacity=64,  # Reduced LSTM size\n",
        "        batch=256,  # Larger batch size\n",
        "        epochs=5,  # Fewer epochs\n",
        "        extra=extra,\n",
        "        lr=0.001,\n",
        "        top_words=500,  # Reduced vocabulary\n",
        "        seed=42,\n",
        "        reverse=rev,\n",
        "        tb_dir='./runs/words/' + name\n",
        "    )\n",
        "    model = train_model(opts, train_data, val_data, test_data, w2i, i2w)\n",
        "    ppl_train = compute_perplexity(model, train_data, opts.batch)\n",
        "    ppl_val = compute_perplexity(model, val_data, opts.batch)\n",
        "    ppl_test = compute_perplexity(model, test_data, opts.batch)\n",
        "    results[name] = {\"Train\": ppl_train, \"Validation\": ppl_val, \"Test\": ppl_test}\n",
        "\n",
        "print(\"\\n=== Summary of Perplexity Results ===\")\n",
        "for config, ppl in results.items():\n",
        "    print(config, ppl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "kss55HrrWh1Z",
        "outputId": "743e4f30-c491-4df9-a10b-4acd355c0531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training configuration: 1-layer_forward ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │         \u001b[38;5;34m640,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m33,024\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_4 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)         │         \u001b[38;5;34m650,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">650,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,323,024\u001b[0m (5.05 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,323,024</span> (5.05 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,323,024\u001b[0m (5.05 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,323,024</span> (5.05 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 58/93 [10:23<06:43, 11.53s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Next-Word Prediction UI  \n",
        "   \n",
        "After training, the function `interactive_next_word` can be used to enter a seed (a word or sequence)\n",
        "and the model will output the next predicted word.\n"
      ],
      "metadata": {
        "id": "tzUNYPqlWkE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, use the last trained model (from the last configuration)\n",
        "print(\"Interactive next-word prediction demo:\")\n",
        "seed_example = [w2i.get(word.lower(), w2i.get('<UNK>')) for word in \"this is\".split()]\n",
        "interactive_next_word(model, seed_example, w2i, i2w)\n"
      ],
      "metadata": {
        "id": "EufBtSBBWlh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary  \n",
        "   \n",
        "In this notebook we have:\n",
        "   \n",
        "- Cloned the repository and loaded the dataset.  \n",
        "- Split the data into train (80%), validation (10%), and test (10%).  \n",
        "- Trained four different LSTM models (1‑layer and 2‑layer; forward and reverse).  \n",
        "- Computed perplexity on all three splits (total 12 results).  \n",
        "- Generated a sentence of length 7 starting with \"love I\" at temperatures 0.1, 1, and 10.  \n",
        "- Computed the probability of the generated sentence and the sentence \"love i cupcakes\".  \n",
        "- Provided an interactive UI for next-word prediction.  \n",
        "   \n",
        "All training progress is logged using TensorBoardX.  \n",
        "   \n",
        "To view your training logs, run in a separate cell:  \n",
        "```python\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./runs/words/\n",
        "```\n",
        "   \n",
        "Happy coding!\n"
      ],
      "metadata": {
        "id": "dy53L4DpWnqv"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}