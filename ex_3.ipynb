{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpp2SnIRfNHUDX74fRnkui",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/deep_learning_neural_networks/blob/main/ex_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MNIST Dataset Preparation**\n",
        "\n",
        "This notebook section preprocesses the MNIST dataset for training, validation, and testing. The workflow includes:\n",
        "1. **Transformations**: Normalize pixel values to [-1, 1] using a predefined transformation pipeline.\n",
        "2. **Dataset Loading**: Load the MNIST training and testing datasets.\n",
        "3. **Data Splitting**: Split the training dataset into 90% training and 10% validation subsets.\n",
        "4. **DataLoader Creation**: Set up batch processing for each subset.\n",
        "\n",
        "Below are reusable functions with proper docstrings to accomplish each step.\n"
      ],
      "metadata": {
        "id": "1mKY9W3_jcBM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-zFxn8TXl5z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def get_transforms():\n",
        "    \"\"\"\n",
        "    Create and return a composition of transformations for preprocessing MNIST dataset.\n",
        "\n",
        "    Returns:\n",
        "        transform (transforms.Compose): Transformation pipeline for MNIST dataset.\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "def load_mnist_dataset(transform):\n",
        "    \"\"\"\n",
        "    Load the MNIST dataset for training and testing.\n",
        "\n",
        "    Args:\n",
        "        transform (transforms.Compose): Transformation pipeline to apply to the dataset.\n",
        "\n",
        "    Returns:\n",
        "        train_dataset (Dataset): MNIST training dataset.\n",
        "        test_dataset (Dataset): MNIST testing dataset.\n",
        "    \"\"\"\n",
        "    train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "    test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def split_train_validation(dataset, train_ratio=0.9):\n",
        "    \"\"\"\n",
        "    Split the training dataset into training and validation subsets.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The full training dataset.\n",
        "        train_ratio (float): Proportion of the dataset to allocate for training.\n",
        "\n",
        "    Returns:\n",
        "        train_subset (Subset): Training subset of the dataset.\n",
        "        val_subset (Subset): Validation subset of the dataset.\n",
        "    \"\"\"\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    return random_split(dataset, [train_size, val_size])\n",
        "\n",
        "def create_dataloaders(train_subset, val_subset, test_dataset, batch_size=50):\n",
        "    \"\"\"\n",
        "    Create DataLoader objects for training, validation, and testing.\n",
        "\n",
        "    Args:\n",
        "        train_subset (Subset): Training subset of the dataset.\n",
        "        val_subset (Subset): Validation subset of the dataset.\n",
        "        test_dataset (Dataset): Test dataset.\n",
        "        batch_size (int): Number of samples per batch.\n",
        "\n",
        "    Returns:\n",
        "        train_loader (DataLoader): DataLoader for the training subset.\n",
        "        val_loader (DataLoader): DataLoader for the validation subset.\n",
        "        test_loader (DataLoader): DataLoader for the test dataset.\n",
        "    \"\"\"\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to load and preprocess the MNIST dataset, split it into subsets, and create DataLoaders.\n",
        "    \"\"\"\n",
        "    transform = get_transforms()\n",
        "    train_dataset, test_dataset = load_mnist_dataset(transform)\n",
        "    train_subset, val_subset = split_train_validation(train_dataset)\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(train_subset, val_subset, test_dataset)\n",
        "\n",
        "    print(f\"Training set size: {len(train_subset)}\")\n",
        "    print(f\"Validation set size: {len(val_subset)}\")\n",
        "    print(f\"Test set size: {len(test_dataset)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}