{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/deep_learning_neural_networks/blob/main/LSTM_Model_Perplexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Language Model Integration with Forward/Reverse Training  \n",
        "   \n",
        "This notebook implements the assignment requirements using the repository code in  \n",
        "`util.py` and `words.py` (which have been modified as described).  \n",
        "   \n",
        "**Tasks completed:**  \n",
        "1. The dataset is split into train (80%), validation (10%), and test (10%) sets.  \n",
        "2. A perplexity metric is computed on each split after training.  \n",
        "3. We support running the LSTM both in the natural (forward) order and in reverse order (using Keras’s `go_backwards` flag).  \n",
        "4. We train 4 LSTM models: one–layer vs. two–layer, and for each a forward and a reverse version.  \n",
        "5. A function is provided that computes the probability of a given sentence from a trained model.  \n",
        "6. A sentence of length 7 starting with \"love I\" is generated at temperatures 0.1, 1, and 10.  \n",
        "7. An interactive UI function allows entering a seed word to obtain the next predicted word.  \n",
        "8. For each model, perplexity is recorded for train, validation, and test sets (12 results total).  \n",
        "9. The probability is computed for the generated sentence and also for the sentence \"love i cupcakes\".\n",
        "\n",
        "All training progress is logged via TensorBoardX.\n"
      ],
      "metadata": {
        "id": "BflmPIL2Ve_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n"
      ],
      "metadata": {
        "id": "neeYvGg5VwEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Cell 1: Package Installation and Repository Cloning\n",
        "\n",
        "This cell installs the required packages, clones the `language_models` repository,\n",
        "and performs necessary module remapping for compatibility.\n",
        "\"\"\"\n",
        "\n",
        "# Install TensorFlow GPU (uncomment if you need GPU support)\n",
        "#!pip install tensorflow-gpu\n",
        "\n",
        "# Install TensorFlow and other required packages\n",
        "!pip install tensorflow\n",
        "!pip install tensorboardX\n",
        "\n",
        "# Clone the language_models repository\n",
        "!git clone https://github.com/GuyKabiri/language_models\n",
        "\n",
        "# Import TensorFlow and SciPy, and remap certain modules for compatibility\n",
        "import tensorflow as tf\n",
        "import scipy\n",
        "import sys\n",
        "import language_models\n",
        "\n",
        "# Remapping modules to use TensorFlow's Keras and SciPy's special functions\n",
        "sys.modules['keras.preprocessing.text'] = tf.keras.preprocessing.text\n",
        "sys.modules['scipy.misc'] = scipy.special\n",
        "\n",
        "\"\"\"\n",
        "Imports for Data Processing, Modeling, and Utilities\n",
        "\n",
        "This cell imports libraries required for data manipulation, model definition,\n",
        "training, and evaluation, along with a utility module from the cloned repository.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Input, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorboardX import SummaryWriter\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from language_models import util\n"
      ],
      "metadata": {
        "id": "VnJOPngRVzAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "1A-JXVX0Z5oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 3: Sequence Generation Function\n",
        "\n",
        "Defines a function to generate a sequence from a trained model using a seed.\n",
        "The function pads the seed to the desired size and samples subsequent tokens\n",
        "based on model predictions and a temperature parameter.\n",
        "\n",
        "Args:\n",
        "    model (Model): Trained TensorFlow Keras model.\n",
        "    seed (np.ndarray): Starting sequence tokens.\n",
        "    size (int): Total length of sequence to generate.\n",
        "    temperature (float): Sampling temperature for randomness (default: 1.0).\n",
        "\n",
        "Returns:\n",
        "    List[int]: Generated sequence as a list of token IDs.\n",
        "\"\"\"\n",
        "def generate_seq(model: Model, seed, size, temperature=1.0):\n",
        "    ls = seed.shape[0]\n",
        "    # Create a padded tokens array with zeros for the remaining length\n",
        "    tokens = np.concatenate([seed, np.zeros(size - ls)])\n",
        "    for i in range(ls, size):\n",
        "        # Predict probabilities for the next token\n",
        "        probs = model.predict(tokens[None, :], verbose=0)\n",
        "        # Sample the next token based on logits and temperature\n",
        "        next_token = util.sample_logits(probs[0, i-1, :], temperature=temperature)\n",
        "        tokens[i] = next_token.item() if isinstance(next_token, np.ndarray) else next_token\n",
        "    return [int(t) for t in tokens]\n"
      ],
      "metadata": {
        "id": "LUHIR8Qn0Vp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 4: Loss and Decoding Functions\n",
        "\n",
        "Defines a sparse loss function wrapper and a decode function to convert token sequences\n",
        "into a human-readable string using a global index-to-word mapping.\n",
        "\n",
        "Functions:\n",
        "    sparse_loss: Computes sparse categorical crossentropy loss.\n",
        "    decode: Converts a sequence of token IDs into a space-separated string.\n",
        "\"\"\"\n",
        "def sparse_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "\n",
        "def decode(seq):\n",
        "    return ' '.join(i2w[id] for id in seq)\n"
      ],
      "metadata": {
        "id": "iif9ovNL0WO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 5: Configuration Setup\n",
        "\n",
        "Defines a configuration class `Args` for hyperparameters and file paths, sets up\n",
        "the TensorBoard SummaryWriter, and initializes the random seed.\n",
        "\n",
        "Attributes:\n",
        "    epochs (int): Number of epochs for training.\n",
        "    embedding_size (int): Size of word embeddings.\n",
        "    out_every (int): Frequency (in epochs) to output logs.\n",
        "    lr (float): Learning rate.\n",
        "    batch (int): Batch size.\n",
        "    task (str): Task identifier (e.g., 'wikisimple').\n",
        "    data (str): Data file path.\n",
        "    lstm_capacity (int): LSTM layer capacity.\n",
        "    max_length: Maximum sentence length.\n",
        "    top_words (int): Vocabulary size.\n",
        "    limit: Optional limit on data size.\n",
        "    tb_dir (str): TensorBoard logging directory.\n",
        "    seed (int): Random seed; negative value triggers random seeding.\n",
        "    extra: Number of extra LSTM layers.\n",
        "\"\"\"\n",
        "class Args:\n",
        "    epochs = 20\n",
        "    embedding_size = 300\n",
        "    out_every = 1\n",
        "    lr = 0.001\n",
        "    batch = 128\n",
        "    task = 'wikisimple'\n",
        "    data = './data'\n",
        "    lstm_capacity = 256\n",
        "    max_length = None\n",
        "    top_words = 10000\n",
        "    limit = None\n",
        "    tb_dir = './runs/words'\n",
        "    seed = -1\n",
        "    extra = None\n",
        "\n",
        "options = Args()\n",
        "tbw = SummaryWriter(log_dir=options.tb_dir)\n",
        "\n",
        "# Initialize random seed for reproducibility\n",
        "if options.seed < 0:\n",
        "    seed_val = random.randint(0, 1000000)\n",
        "    print('Random seed:', seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    options.seed = seed_val\n",
        "else:\n",
        "    np.random.seed(options.seed)\n"
      ],
      "metadata": {
        "id": "vxR2lqFy0aPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 6: Data Loading\n",
        "\n",
        "Loads the dataset based on the specified task. For the 'wikisimple' task, the dataset\n",
        "is loaded from a predefined path within the utility module. If 'file' is selected, it\n",
        "loads data from the provided file path.\n",
        "\n",
        "Returns:\n",
        "    X: List of sequences (each sequence is a list of token IDs).\n",
        "    w2i: Dictionary mapping words to their indices.\n",
        "    i2w: Dictionary mapping indices to their words.\n",
        "\n",
        "Raises:\n",
        "    Exception: If the specified task is not recognized.\n",
        "\"\"\"\n",
        "if options.task == 'wikisimple':\n",
        "    X, w2i, i2w = util.load_words(util.DIR + '/datasets/wikisimple.txt', vocab_size=options.top_words, limit=options.limit)\n",
        "elif options.task == 'file':\n",
        "    X, w2i, i2w = util.load_words(options.data, vocab_size=options.top_words, limit=options.limit)\n",
        "else:\n",
        "    raise Exception(f'Task {options.task} not recognized.')\n"
      ],
      "metadata": {
        "id": "C8N5m4aq0ctZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 7: Data Splitting\n",
        "\n",
        "Splits the dataset into training, testing, and validation sets.\n",
        "First, 80% of the data is allocated to training, and 20% is reserved for testing+validation.\n",
        "Then, the test+validation set is split equally into test and validation sets.\n",
        "\"\"\"\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, X, test_size=0.2, random_state=options.seed)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=options.seed)\n"
      ],
      "metadata": {
        "id": "x11wj6yG0eeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 8: Data Padding and Sequence Length Determination\n",
        "\n",
        "Determines the maximum sequence length from the dataset and pads all batches\n",
        "to ensure uniform sequence lengths using the utility function.\n",
        "\"\"\"\n",
        "x_max_len = max(len(seq) for seq in X)\n",
        "numwords = len(i2w)\n",
        "print('Max sequence length:', x_max_len)\n",
        "print(numwords, 'distinct words')\n",
        "\n",
        "# Pad the sequences in training, validation, and test sets with an end-of-sequence token\n",
        "X_train = util.batch_pad(X_train, options.batch, add_eos=True)\n",
        "X_val = util.batch_pad(X_val, options.batch, add_eos=True)\n",
        "X_test = util.batch_pad(X_test, options.batch, add_eos=True)\n",
        "print('Finished data loading. ', sum([b.shape[0] for b in X_train]), ' sentences loaded')\n"
      ],
      "metadata": {
        "id": "9us57IrV0gic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 9: LSTM Model Creation Function\n",
        "\n",
        "Creates an LSTM-based model for language modeling. Optionally adds extra LSTM layers\n",
        "if specified. The model uses an Embedding layer, one or more LSTM layers, and a\n",
        "TimeDistributed Dense layer to output logits for each time step.\n",
        "\n",
        "Args:\n",
        "    extra (int): Number of additional LSTM layers to add (default: None).\n",
        "    lr (float): Learning rate for the optimizer.\n",
        "\n",
        "Returns:\n",
        "    model (Model): Compiled TensorFlow Keras model.\n",
        "\"\"\"\n",
        "def create_lstm_model(extra=None, lr=0.001):\n",
        "    inp = Input(shape=(None, ))\n",
        "    embedding_layer = Embedding(numwords, options.embedding_size, input_length=None)\n",
        "    embedded = embedding_layer(inp)\n",
        "    decoder_lstm = LSTM(options.lstm_capacity, return_sequences=True)\n",
        "    h = decoder_lstm(embedded)\n",
        "\n",
        "    # Optionally add extra LSTM layers\n",
        "    if extra is not None:\n",
        "        for _ in range(extra):\n",
        "            h = LSTM(options.lstm_capacity, return_sequences=True)(h)\n",
        "\n",
        "    dense_layer = Dense(numwords, activation='linear')\n",
        "    out = TimeDistributed(dense_layer)(h)\n",
        "\n",
        "    model = Model(inp, out)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(opt, sparse_loss)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "mOpx0Uyj0iXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 10: Model Training Function\n",
        "\n",
        "Trains a given model on the training data for a specified number of epochs.\n",
        "Supports both forward and backward training directions by appropriately\n",
        "shifting and padding input sequences.\n",
        "\n",
        "Args:\n",
        "    model (Model): The TensorFlow Keras model to be trained.\n",
        "    X_train (list): List of padded training data batches.\n",
        "    direction (str): Training direction ('forward' or 'backward').\n",
        "\n",
        "Logs:\n",
        "    Batch loss is logged to TensorBoard.\n",
        "\"\"\"\n",
        "def train_model(model, X_train, direction='forward'):\n",
        "    epoch = 0\n",
        "    instances_seen = 0\n",
        "    while epoch < options.epochs:\n",
        "        for batch in tqdm(X_train):\n",
        "            n, l = batch.shape\n",
        "            if direction == 'backward':\n",
        "                batch_reversed = np.flip(batch, axis=1)\n",
        "                batch_shifted = np.concatenate([np.ones((n, 1)), batch_reversed], axis=1)\n",
        "                batch_out = np.concatenate([batch_reversed, np.zeros((n, 1))], axis=1)\n",
        "            else:\n",
        "                batch_shifted = np.concatenate([np.ones((n, 1)), batch], axis=1)\n",
        "                batch_out = np.concatenate([batch, np.zeros((n, 1))], axis=1)\n",
        "\n",
        "            loss = model.train_on_batch(batch_shifted, batch_out[:, :, None])\n",
        "            instances_seen += n\n",
        "            tbw.add_scalar('lm/batch-loss', float(loss), instances_seen)\n",
        "        print(loss)\n",
        "        epoch += 1\n"
      ],
      "metadata": {
        "id": "EqMVnsOe0lcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 11: Perplexity Computation Function\n",
        "\n",
        "Computes the perplexity of the model on a given dataset (training, validation, or test)\n",
        "by evaluating the average loss per token and exponentiating it.\n",
        "\n",
        "Args:\n",
        "    model (Model): Trained TensorFlow Keras model.\n",
        "    data_batches (list): List of padded data batches.\n",
        "    direction (str): Direction for sequence processing ('forward' or 'backward').\n",
        "\n",
        "Returns:\n",
        "    perplexity (float): The computed perplexity score for the dataset.\n",
        "\"\"\"\n",
        "def compute_perplexity(model, data_batches, direction='forward'):\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    total_batches = len(data_batches)\n",
        "    with tqdm(total=total_batches, desc=\"Computing Perplexity\") as pbar:\n",
        "        for batch in data_batches:\n",
        "            n, l = batch.shape\n",
        "            if direction == 'backward':\n",
        "                batch_reversed = np.flip(batch, axis=1)\n",
        "                batch_shifted = np.concatenate([np.ones((n, 1)), batch_reversed], axis=1)\n",
        "                batch_out = np.concatenate([batch_reversed, np.zeros((n, 1))], axis=1)\n",
        "            else:\n",
        "                batch_shifted = np.concatenate([np.ones((n, 1)), batch], axis=1)\n",
        "                batch_out = np.concatenate([batch, np.zeros((n, 1))], axis=1)\n",
        "\n",
        "            loss = model.evaluate(batch_shifted, batch_out[:, :, None], verbose=0)\n",
        "            non_padding_tokens = np.sum(batch_out != 0)\n",
        "            total_loss += loss * non_padding_tokens\n",
        "            total_tokens += non_padding_tokens\n",
        "            pbar.update(1)\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = np.exp(avg_loss)\n",
        "    return perplexity\n"
      ],
      "metadata": {
        "id": "yu2p4gJE0nVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 12: Model Definitions\n",
        "\n",
        "Defines four LSTM-based models with varying configurations:\n",
        "  - Model 1: 1 LSTM layer with forward training.\n",
        "  - Model 2: 1 LSTM layer with backward training.\n",
        "  - Model 3: 2 LSTM layers with forward training.\n",
        "  - Model 4: 2 LSTM layers with backward training.\n",
        "\n",
        "The models are appended to a list and their architectures are summarized.\n",
        "\"\"\"\n",
        "models = []\n",
        "# Model 1: 1 LSTM layer, forward training\n",
        "model1 = create_lstm_model(lr=0.01)\n",
        "models.append(model1)\n",
        "\n",
        "# Model 2: 1 LSTM layer, backward training\n",
        "model2 = create_lstm_model(lr=0.01)\n",
        "models.append(model2)\n",
        "\n",
        "# Model 3: 2 LSTM layers, forward training\n",
        "model3 = create_lstm_model(extra=1, lr=0.001)\n",
        "models.append(model3)\n",
        "\n",
        "# Model 4: 2 LSTM layers, backward training\n",
        "model4 = create_lstm_model(extra=1, lr=0.001)\n",
        "models.append(model4)\n",
        "\n",
        "# Print model summaries\n",
        "for model in models:\n",
        "    model.summary()\n"
      ],
      "metadata": {
        "id": "pesDDF5j0pTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 13: Models Training\n",
        "\n",
        "Trains each model using the training data. The training direction is determined by the model's index:\n",
        "  - Even-indexed models use forward training.\n",
        "  - Odd-indexed models use backward training.\n",
        "\n",
        "Progress and loss are printed during training.\n",
        "\"\"\"\n",
        "print(\"Model 1 - 1 LSTM layer | forward training\")\n",
        "print(\"Model 2 - 1 LSTM layer | backward training\")\n",
        "print(\"Model 3 - 2 LSTM layer | forward training\")\n",
        "print(\"Model 4 - 2 LSTM layer | backward training\")\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    direction = 'forward' if i % 2 == 0 else 'backward'\n",
        "    print(f\"Training model {i+1}\")\n",
        "    train_model(model, X_train, direction)\n",
        "\n",
        "print(\"Training finished\")\n"
      ],
      "metadata": {
        "id": "jRpskdP70rHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 14: Perplexity Evaluation\n",
        "\n",
        "Computes and prints the perplexity for each model on the training, validation, and test sets.\n",
        "Perplexity is a common metric in language modeling to evaluate how well a probability model predicts a sample.\n",
        "\"\"\"\n",
        "train_perplexities = []\n",
        "val_perplexities = []\n",
        "test_perplexities = []\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    direction = 'forward' if i % 2 == 0 else 'backward'\n",
        "    train_perplexity = compute_perplexity(model, X_train, direction)\n",
        "    val_perplexity = compute_perplexity(model, X_val, direction)\n",
        "    test_perplexity = compute_perplexity(model, X_test, direction)\n",
        "    train_perplexities.append(train_perplexity)\n",
        "    val_perplexities.append(val_perplexity)\n",
        "    test_perplexities.append(test_perplexity)\n",
        "\n",
        "print('\\n')\n",
        "for i in range(4):\n",
        "    print(f'Model {i+1}:')\n",
        "    print(f\"Train Perplexity : {train_perplexities[i]}\")\n",
        "    print(f\"Validation Perplexity : {val_perplexities[i]}\")\n",
        "    print(f\"Test Perplexity : {test_perplexities[i]}\")"
      ],
      "metadata": {
        "id": "M3S8AVG90tAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./runs/words/"
      ],
      "metadata": {
        "id": "YSoOFXrUzYJ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}