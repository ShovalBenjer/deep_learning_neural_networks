{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/deep_learning_neural_networks/blob/main/LSTM_Model_Perplexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Language Model Integration with Forward/Reverse Training  \n",
        "   \n",
        "This notebook implements the assignment requirements using the repository code in  \n",
        "`util.py` and `words.py` (which have been modified as described).  \n",
        "   \n",
        "**Tasks completed:**  \n",
        "1. The dataset is split into train (80%), validation (10%), and test (10%) sets.  \n",
        "2. A perplexity metric is computed on each split after training.  \n",
        "3. We support running the LSTM both in the natural (forward) order and in reverse order (using Keras’s `go_backwards` flag).  \n",
        "4. We train 4 LSTM models: one–layer vs. two–layer, and for each a forward and a reverse version.  \n",
        "5. A function is provided that computes the probability of a given sentence from a trained model.  \n",
        "6. A sentence of length 7 starting with \"love I\" is generated at temperatures 0.1, 1, and 10.  \n",
        "7. An interactive UI function allows entering a seed word to obtain the next predicted word.  \n",
        "8. For each model, perplexity is recorded for train, validation, and test sets (12 results total).  \n",
        "9. The probability is computed for the generated sentence and also for the sentence \"love i cupcakes\".\n",
        "\n",
        "All training progress is logged via TensorBoardX.\n"
      ],
      "metadata": {
        "id": "BflmPIL2Ve_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n"
      ],
      "metadata": {
        "id": "neeYvGg5VwEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX\n",
        "!pip install nltk\n",
        "!pip install keras-preprocessing\n",
        "!pip install tensorflow-gpu\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install keras-preprocessing-gpu\n",
        "# %tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "!pip install tensorboardX\n",
        "# !pip install language_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnJOPngRVzAj",
        "outputId": "d9bb59bb-37ef-4a81-df13-967a3ed8877f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (4.25.6)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from keras-preprocessing) (1.26.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from keras-preprocessing) (1.17.0)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-preprocessing\n",
            "Successfully installed keras-preprocessing-1.1.2\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement keras-preprocessing-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for keras-preprocessing-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (4.25.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A-JXVX0Z5oV",
        "outputId": "b64b5707-77e5-42af-cec9-4f95b947ca34"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPU Available: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone the Repository  \n",
        "   \n",
        "We now clone the repository from GitHub and check out the correct branch.\n"
      ],
      "metadata": {
        "id": "5ChsewebV1Oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShovalBenjer/deep_learning_neural_networks.git\n",
        "%cd deep_learning_neural_networks\n",
        "!git checkout language-models-integration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "323BLKz4V3QQ",
        "outputId": "6ff08ab5-01fe-4e4a-c72d-28294720d5d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep_learning_neural_networks'...\n",
            "remote: Enumerating objects: 199, done.\u001b[K\n",
            "remote: Counting objects: 100% (199/199), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 199 (delta 104), reused 103 (delta 55), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (199/199), 21.32 MiB | 12.13 MiB/s, done.\n",
            "Resolving deltas: 100% (104/104), done.\n",
            "/content/deep_learning_neural_networks\n",
            "Branch 'language-models-integration' set up to track remote branch 'language-models-integration' from 'origin'.\n",
            "Switched to a new branch 'language-models-integration'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Setup  \n",
        "   \n",
        "We import necessary modules from Keras, tensorboardX, and our own `util.py` and `words.py`.\n"
      ],
      "metadata": {
        "id": "WAXw5A7pV5Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, numpy as np, keras, keras.backend as K, nltk\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Input, Dense\n",
        "from keras.models import Model\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "nltk.download('punkt')\n",
        "from keras.datasets import imdb\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tqdm import tqdm\n",
        "from argparse import ArgumentParser\n",
        "from tensorboardX import SummaryWriter\n",
        "CHECK = 5\n",
        "\n",
        "# Import our custom modules (make sure util.py and words.py are in the repo)\n",
        "import util\n",
        "import words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C18Z59xYV56l",
        "outputId": "fa1c9e1b-d9c5-46fe-da5d-adf40fd1258a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions  \n",
        "   \n",
        "The following helper functions are defined:\n",
        "   \n",
        "- **split_data:** Splits a numpy array of padded sentences into train/val/test sets (80–10–10).  \n",
        "- **compute_perplexity:** Computes average loss on a dataset and returns perplexity = exp(loss).  \n",
        "- **sentence_probability:** Computes the probability (and log probability) of a sentence given a trained model.  \n",
        "- **interactive_next_word:** A simple UI function that takes a seed sequence and prints the next predicted word.\n"
      ],
      "metadata": {
        "id": "x82C2IIrWB9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Splitting the Data  \n",
        "   \n",
        "We load the dataset using `util.load_words` from the file `datasets/wikisimple.txt` and then pad the sentences.  \n",
        "Next, we concatenate all batches into one array and split it (80% train, 10% validation, 10% test).\n"
      ],
      "metadata": {
        "id": "ModHvdIZWIad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Split a numpy array (of shape [num_sentences, seq_length]) into train, validation, and test arrays.\n",
        "    \"\"\"\n",
        "    indices = np.arange(data.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    n = len(indices)\n",
        "    train_end = int(train_ratio * n)\n",
        "    val_end = int((train_ratio + val_ratio) * n)\n",
        "    train_data = data[indices[:train_end]]\n",
        "    val_data = data[indices[train_end:val_end]]\n",
        "    test_data = data[indices[val_end:]]\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def compute_perplexity(model, data, batch_size=128):\n",
        "    \"\"\"\n",
        "    Compute perplexity on a dataset.\n",
        "\n",
        "    The loss is calculated using sparse categorical crossentropy.\n",
        "    Perplexity is computed as exp(average_loss).\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    for i in range(0, data.shape[0], batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        n = batch.shape[0]\n",
        "        batch_in = np.concatenate([np.ones((n, 1), dtype='int32'), batch], axis=1)\n",
        "        batch_out = np.concatenate([batch, np.zeros((n, 1), dtype='int32')], axis=1)\n",
        "        loss = model.test_on_batch(batch_in, batch_out[:,:,None])\n",
        "        losses.append(loss)\n",
        "    avg_loss = np.mean(losses)\n",
        "    return np.exp(avg_loss)\n",
        "\n",
        "def sentence_probability(sentence, model, w2i, i2w):\n",
        "    \"\"\"\n",
        "    Compute the probability and log-probability of a given sentence.\n",
        "\n",
        "    The sentence is tokenized by spaces; unknown words are replaced by <UNK>.\n",
        "    The model is assumed to predict the next word given the previous tokens.\n",
        "    \"\"\"\n",
        "    words_in = sentence.strip().split()\n",
        "    # Use <UNK> index if word not found\n",
        "    unk = w2i.get('<UNK>', 2)\n",
        "    token_ids = [w2i.get(word.lower(), unk) for word in words_in]\n",
        "    # Prepend <START> token (assumed index 1)\n",
        "    token_ids = [1] + token_ids\n",
        "    token_ids = np.array(token_ids)[None, :]  # shape (1, seq_len)\n",
        "    logits = model.predict(token_ids)\n",
        "    # Apply softmax to obtain probabilities at each time step\n",
        "    probs = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n",
        "    p = 1.0\n",
        "    logp = 0.0\n",
        "    # For t=1 to end, probability assigned to token at position t given context t-1\n",
        "    for t in range(1, token_ids.shape[1]):\n",
        "        prob = probs[0, t-1, token_ids[0, t]]\n",
        "        p *= prob\n",
        "        logp += np.log(prob + 1e-10)\n",
        "    return p, logp\n",
        "\n",
        "def interactive_next_word(model, seed_seq, w2i, i2w):\n",
        "    \"\"\"\n",
        "    Given a seed sequence (list of word indices), generate one additional word using the model.\n",
        "    \"\"\"\n",
        "    seed_seq = np.array(seed_seq)\n",
        "    seed_seq = np.insert(seed_seq, 0, 1)  # Prepend <START> token (index 1)\n",
        "    gen = words.generate_seq(model, seed_seq, size=seed_seq.shape[0]+1, temperature=1.0)\n",
        "    next_idx = gen[-1]\n",
        "    seed_words = ' '.join(i2w[str(idx)] for idx in seed_seq[1:])\n",
        "    print(\"Seed: \", seed_words)\n",
        "    print(\"Next predicted word: \", i2w[str(next_idx)])\n"
      ],
      "metadata": {
        "id": "GigfQphuWGOo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building and Training Functions  \n",
        "   \n",
        "We define two functions:  \n",
        "   \n",
        "- **build_model:** Constructs the Keras model. It uses the parameter `reverse` to decide whether the LSTM layers operate in reverse order (by setting `go_backwards=True`). An extra LSTM layer is added if requested.  \n",
        "- **train_model:** Trains the model on the training set in a custom loop, logs loss via TensorBoardX, and after training computes perplexity on the train, validation, and test sets. It also demonstrates sample generation.\n"
      ],
      "metadata": {
        "id": "mTMJc5u4WYTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(numwords, lstm_capacity, extra, reverse):\n",
        "    \"\"\"\n",
        "    Build and return a Keras Model for language modeling.\n",
        "\n",
        "    :param numwords: Size of the vocabulary.\n",
        "    :param lstm_capacity: The dimensionality of the LSTM hidden state.\n",
        "    :param extra: Number of extra LSTM layers (None means only one layer).\n",
        "    :param reverse: Boolean flag; if True, LSTM layers use go_backwards=True.\n",
        "    :return: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    inp = Input(shape=(None,))\n",
        "    embed = Embedding(numwords, lstm_capacity)\n",
        "    x = embed(inp)\n",
        "    # First LSTM layer with reverse flag\n",
        "    x = LSTM(lstm_capacity, return_sequences=True, go_backwards=reverse)(x)\n",
        "    # Extra LSTM layers if any\n",
        "    if extra is not None:\n",
        "        for _ in range(extra):\n",
        "            x = LSTM(lstm_capacity, return_sequences=True, go_backwards=reverse)(x)\n",
        "    dense = Dense(numwords, activation='linear')\n",
        "    out = TimeDistributed(dense)(x)\n",
        "    model = Model(inp, out)\n",
        "    return model\n",
        "\n",
        "def train_model(options, train_data, val_data, test_data, w2i, i2w):\n",
        "    \"\"\"\n",
        "    Build and train a language model with the specified options.\n",
        "    Logs training progress via TensorBoardX.\n",
        "    After training, computes perplexity on train, validation, and test sets,\n",
        "    and generates sample sentences.\n",
        "\n",
        "    :param options: An options object with training hyperparameters.\n",
        "    :param train_data, val_data, test_data: Numpy arrays of shape [num_sentences, seq_length].\n",
        "    :param w2i, i2w: Word-to-index and index-to-word dictionaries.\n",
        "    :return: The trained Keras model.\n",
        "    \"\"\"\n",
        "    writer = SummaryWriter(logdir=options.tb_dir)\n",
        "    np.random.seed(options.seed)\n",
        "\n",
        "    numwords = len(i2w)\n",
        "    model = build_model(numwords, options.lstm_capacity, options.extra, options.reverse)\n",
        "    # Use \"learning_rate\" instead of \"lr\" below.\n",
        "    opt = keras.optimizers.Adam(learning_rate=options.lr)\n",
        "    model.compile(opt, words.sparse_loss)\n",
        "    model.summary()\n",
        "\n",
        "    instances_seen = 0\n",
        "    for epoch in range(options.epochs):\n",
        "        # Shuffle training data each epoch\n",
        "        indices = np.arange(train_data.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        train_data = train_data[indices]\n",
        "        for i in range(0, train_data.shape[0], options.batch):\n",
        "            batch = train_data[i:i+options.batch]\n",
        "            n = batch.shape[0]\n",
        "            batch_in = np.concatenate([np.ones((n, 1), dtype='int32'), batch], axis=1)\n",
        "            batch_out = np.concatenate([batch, np.zeros((n, 1), dtype='int32')], axis=1)\n",
        "            loss = model.train_on_batch(batch_in, batch_out[:,:,None])\n",
        "            instances_seen += n\n",
        "            writer.add_scalar('lm/train_batch_loss', float(loss), instances_seen)\n",
        "        print(\"Epoch {} complete\".format(epoch+1))\n",
        "        # Generate sample sentences at various temperatures\n",
        "        for temp in [0.0, 0.9, 1.0, 1.1, 1.2]:\n",
        "            print(\"### TEMP\", temp)\n",
        "            for _ in range(3):\n",
        "                b = random.choice(train_data)\n",
        "                seed = b[0, :min(20, b.shape[1])]\n",
        "                seed = np.insert(seed, 0, 1)  # Prepend <START>\n",
        "                gen = words.generate_seq(model, seed, 60, temperature=temp)\n",
        "                def decode(seq):\n",
        "                    return ' '.join(i2w[str(i)] for i in seq)\n",
        "                print('Seed:', decode(seed))\n",
        "                print('Generated:', decode(gen[len(seed):]))\n",
        "    writer.close()\n",
        "\n",
        "    ppl_train = compute_perplexity(model, train_data, options.batch)\n",
        "    ppl_val = compute_perplexity(model, val_data, options.batch)\n",
        "    ppl_test = compute_perplexity(model, test_data, options.batch)\n",
        "\n",
        "    print(\"Perplexity (Train): {:.2f}\".format(ppl_train))\n",
        "    print(\"Perplexity (Validation): {:.2f}\".format(ppl_val))\n",
        "    print(\"Perplexity (Test): {:.2f}\".format(ppl_test))\n",
        "\n",
        "    # Generate sentence of length 7 starting with \"love I\" at different temperatures\n",
        "    seed_words = \"love I\".split()\n",
        "    seed_ids = [w2i.get(word.lower(), w2i.get('<UNK>')) for word in seed_words]\n",
        "    seed_ids = np.array(seed_ids)\n",
        "    print(\"\\nSentence generation (length=7) starting with 'love I':\")\n",
        "    for temp in [0.1, 1.0, 10.0]:\n",
        "        gen = words.generate_seq(model, np.insert(seed_ids, 0, 1), size=7, temperature=temp)\n",
        "        def decode(seq):\n",
        "            return ' '.join(i2w[str(i)] for i in seq)\n",
        "        print(\"Temperature {}: {}\".format(temp, decode(gen)))\n",
        "\n",
        "    # Compute probability for the generated sentence and for \"love i cupcakes\"\n",
        "    def decode(seq):\n",
        "        return ' '.join(i2w[str(i)] for i in seq)\n",
        "    generated_sentence = decode(gen)\n",
        "    p, logp = sentence_probability(generated_sentence, model, w2i, i2w)\n",
        "    print(\"\\nProbability for generated sentence '{}': {:.2e} (log={:.2f})\".format(generated_sentence, p, logp))\n",
        "\n",
        "    sentence2 = \"love i cupcakes\"\n",
        "    p2, logp2 = sentence_probability(sentence2, model, w2i, i2w)\n",
        "    print(\"Probability for sentence 'love i cupcakes': {:.2e} (log={:.2f})\".format(p2, logp2))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "XuVkVCAsWZQo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Options Class and Helper to Set Options  \n",
        "   \n",
        "We define a simple options container and a function to generate options for training.\n"
      ],
      "metadata": {
        "id": "tGyfF5FKWbpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Options:\n",
        "    pass\n",
        "\n",
        "def get_options(lstm_capacity=256, batch=128, epochs=10, extra=None, lr=0.001,\n",
        "                top_words=10000, limit=None, seed=42, reverse=False, tb_dir='./runs/words'):\n",
        "    opt = Options()\n",
        "    opt.lstm_capacity = lstm_capacity\n",
        "    opt.batch = batch\n",
        "    opt.epochs = epochs\n",
        "    opt.extra = extra      # extra LSTM layers (None means only one LSTM layer)\n",
        "    opt.lr = lr\n",
        "    opt.top_words = top_words\n",
        "    opt.limit = limit\n",
        "    opt.seed = seed\n",
        "    opt.reverse = reverse  # if True, LSTM layers are trained in reverse order\n",
        "    opt.tb_dir = tb_dir\n",
        "    opt.task = 'wikisimple'\n",
        "    opt.data = './data'\n",
        "    return opt\n"
      ],
      "metadata": {
        "id": "0IJHVMh1Wc-r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Four Models and Recording Perplexity  \n",
        "   \n",
        "We now run experiments for four configurations:  \n",
        "1. 1-layer forward (extra=None, reverse=False)  \n",
        "2. 1-layer reverse (extra=None, reverse=True)  \n",
        "3. 2-layer forward (extra=1, reverse=False)  \n",
        "4. 2-layer reverse (extra=1, reverse=True)  \n",
        "   \n",
        "For each model we train on the training set, compute perplexity on train, validation and test sets, and record the results.\n"
      ],
      "metadata": {
        "id": "z2-Fkj9PWer7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data (x is a list of lists of integers)\n",
        "x, w2i, i2w = util.load_words(os.path.join(util.DIR, 'datasets', 'wikisimple.txt'),\n",
        "                               vocab_size=10000, limit=None)\n",
        "print(\"Number of sentences loaded (before batching):\", len(x))\n",
        "\n",
        "# Compute the global maximum sentence length\n",
        "global_max_len = max(len(sentence) for sentence in x)\n",
        "print(\"Global maximum sentence length:\", global_max_len)\n",
        "\n",
        "# Pad all sentences to the same length\n",
        "from keras_preprocessing.sequence import pad_sequences  # or from tensorflow.keras.preprocessing.sequence\n",
        "all_data = pad_sequences(x, maxlen=global_max_len, dtype='int32', padding='post', truncating='post')\n",
        "print(\"Total sentences (after global padding):\", all_data.shape[0])\n",
        "\n",
        "# Now split the padded array into train, validation, and test sets\n",
        "train_data, val_data, test_data = split_data(all_data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)\n",
        "print(\"Train: {}, Validation: {}, Test: {}\".format(train_data.shape[0], val_data.shape[0], test_data.shape[0]))\n"
      ],
      "metadata": {
        "id": "dVC_1drjbDDl",
        "outputId": "9a475589-5492-4391-8c40-b45978895cef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw data read\n",
            "Number of sentences loaded (before batching): 29741\n",
            "Global maximum sentence length: 124\n",
            "Total sentences (after global padding): 29741\n",
            "Train: 23792, Validation: 2974, Test: 2975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "configs = [\n",
        "    (\"1-layer_forward\", None, False),\n",
        "    (\"1-layer_reverse\", None, True),\n",
        "    (\"2-layer_forward\", 1, False),\n",
        "    (\"2-layer_reverse\", 1, True)\n",
        "]\n",
        "\n",
        "for name, extra, rev in configs:\n",
        "    print(\"\\n=== Training configuration: {} ===\".format(name))\n",
        "    opts = get_options(lstm_capacity=256, batch=128, epochs=10, extra=extra,\n",
        "                       lr=0.001, top_words=10000, seed=42, reverse=rev, tb_dir='./runs/words/' + name)\n",
        "    model = train_model(opts, train_data, val_data, test_data, w2i, i2w)\n",
        "    ppl_train = compute_perplexity(model, train_data, opts.batch)\n",
        "    ppl_val = compute_perplexity(model, val_data, opts.batch)\n",
        "    ppl_test = compute_perplexity(model, test_data, opts.batch)\n",
        "    results[name] = {\"Train\": ppl_train, \"Validation\": ppl_val, \"Test\": ppl_test}\n",
        "\n",
        "print(\"\\n=== Summary of Perplexity Results ===\")\n",
        "for config, ppl in results.items():\n",
        "    print(config, ppl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "kss55HrrWh1Z",
        "outputId": "3effe147-b24b-4224-cbd7-71ca364adef2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training configuration: 1-layer_forward ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m2,560,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m525,312\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_1 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)         │       \u001b[38;5;34m2,570,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,655,312\u001b[0m (21.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,655,312</span> (21.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,655,312\u001b[0m (21.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,655,312</span> (21.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'keras.backend' has no attribute 'sparse_categorical_crossentropy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f000ddd08e61>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     opts = get_options(lstm_capacity=256, batch=128, epochs=10, extra=extra,\n\u001b[1;32m     12\u001b[0m                        lr=0.001, top_words=10000, seed=42, reverse=rev, tb_dir='./runs/words/' + name)\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mppl_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mppl_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-84b0b52823d0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(options, train_data, val_data, test_data, w2i, i2w)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mbatch_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mbatch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0minstances_seen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lm/train_batch_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances_seen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_execution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 ):\n\u001b[0;32m--> 227\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mone_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;34m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             outputs = reduce_per_replica(\n\u001b[1;32m    115\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             loss = self._compute_loss(\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/trainer.py\u001b[0m in \u001b[0;36m_compute_loss\u001b[0;34m(self, x, y, y_pred, sample_weight, training)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \"\"\"\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_loss_has_training_arg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             return self.compute_loss(\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/compile_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/compile_utils.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             loss_value = ops.cast(\n\u001b[0;32m--> 700\u001b[0;31m                 \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m             )\n\u001b[1;32m    702\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/losses/loss.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     65\u001b[0m             )\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mout_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_keras_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_y_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_y_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep_learning_neural_networks/words.py\u001b[0m in \u001b[0;36msparse_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msparse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'sparse_categorical_crossentropy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Next-Word Prediction UI  \n",
        "   \n",
        "After training, the function `interactive_next_word` can be used to enter a seed (a word or sequence)\n",
        "and the model will output the next predicted word.\n"
      ],
      "metadata": {
        "id": "tzUNYPqlWkE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, use the last trained model (from the last configuration)\n",
        "print(\"Interactive next-word prediction demo:\")\n",
        "seed_example = [w2i.get(word.lower(), w2i.get('<UNK>')) for word in \"this is\".split()]\n",
        "interactive_next_word(model, seed_example, w2i, i2w)\n"
      ],
      "metadata": {
        "id": "EufBtSBBWlh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary  \n",
        "   \n",
        "In this notebook we have:\n",
        "   \n",
        "- Cloned the repository and loaded the dataset.  \n",
        "- Split the data into train (80%), validation (10%), and test (10%).  \n",
        "- Trained four different LSTM models (1‑layer and 2‑layer; forward and reverse).  \n",
        "- Computed perplexity on all three splits (total 12 results).  \n",
        "- Generated a sentence of length 7 starting with \"love I\" at temperatures 0.1, 1, and 10.  \n",
        "- Computed the probability of the generated sentence and the sentence \"love i cupcakes\".  \n",
        "- Provided an interactive UI for next-word prediction.  \n",
        "   \n",
        "All training progress is logged using TensorBoardX.  \n",
        "   \n",
        "To view your training logs, run in a separate cell:  \n",
        "```python\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./runs/words/\n",
        "```\n",
        "   \n",
        "Happy coding!\n"
      ],
      "metadata": {
        "id": "dy53L4DpWnqv"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}