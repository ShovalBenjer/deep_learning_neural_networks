{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPXBOprBGpJU8oGfQTz37qs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/deep_learning_neural_networks/blob/main/Deep_exc_2_adir_shoval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TL;DR:**\n",
        "\n",
        "**Collaborators: Shoval Benjer 319037404, Adir Amar 209017755**\n",
        "\n",
        "This assignment focuses on Implementing a neural network in PyTorch to solve the XOR problem.\n",
        "Exploring configurations of the network by specific instructions.\n",
        "Documenting results with reproducible experiments and clear outputs."
      ],
      "metadata": {
        "id": "4v5p7MnNcqZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **setup:**\n",
        "\n",
        "\n",
        "To run this code, you'll need the following requirements:\n",
        "\n",
        "Python 3.x\n",
        "\n",
        "PyTorch\n",
        "\n",
        "NumPy\n",
        "\n",
        "Pandas\n",
        "\n",
        "You can install these requirements using pip:\n",
        "\n",
        "`!pip install torch numpy pandas`\n",
        "\n",
        "To run the code:\n",
        "\n",
        "1. Copy the provided code into a Python file (e.g., xor_network.py)\n",
        "2. Run the file using Python:\n",
        "`python xor_network.py`\n",
        "\n",
        "If you need to run this in VLab:\n",
        "Log in to your VLab account\n",
        "Open a terminal\n",
        "**Ensure the required packages are installed** (use the pip command above if needed)\n",
        "Navigate to the directory containing your Python file\n",
        "Run the file using Python as described above\n",
        "The code will automatically run experiments for k=1 (with bypass), k=2, and k=4, displaying the weights, biases, loss values, and truth tables for each configuration.\n",
        "\n",
        "Note: The code uses a low temperature (0.001) for the BTU/sigmoid function as requested in the assignment. No additional setup is required beyond having the necessary Python packages installed."
      ],
      "metadata": {
        "id": "IQg-LYwlx0cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch matplotlib"
      ],
      "metadata": {
        "id": "oc494TvfoKhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0fadf6-7b0e-4988-eb38-9ebfd7e83171"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import statistics\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Pn4So9pxnDnL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare XOR dataset\n",
        "train_x = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=torch.float32)\n",
        "train_y = torch.tensor([[0.], [1.], [1.], [0.]], dtype=torch.float32)\n",
        "val_x = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.], [1., 0.1], [1., 0.9], [0.9, 0.9], [0.1, 0.9]], dtype=torch.float32)\n",
        "val_y = torch.tensor([[0.], [1.], [1.], [0.], [1.], [0.], [0.], [1.]], dtype=torch.float32)\n",
        "\n",
        "# Experiments\n",
        "experiments = [\n",
        "    (0.1, 2, False), (0.1, 2, True), (0.1, 4, False), (0.1, 4, True),\n",
        "    (0.01, 2, False), (0.01, 2, True), (0.01, 4, False), (0.01, 4, True),\n",
        "    (1.0, 1, True)\n",
        "]"
      ],
      "metadata": {
        "id": "u3M4fFOD6an_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuration and Data Definition**\n",
        "Here we define the training and validation sets. According to the assignment,\n",
        "\n",
        "we train a small MLP to learn the XOR function, with a validation set that includes some additional points.\n",
        "\n",
        "**Training set:** the standard XOR pattern\n",
        "\n",
        "**Validation set:** includes the training points plus additional specified points.\n",
        "\n",
        "**Stopping criteria:**\n",
        "\n",
        "1) Stop successfully if the validation loss hasn't improved by more than 0.0001 over the last 10 epoch AND val_loss < 0.2.\n",
        "\n",
        "2) Stop with failure if we reach more than 40,000 epochs with no success condition met.\n",
        "\n",
        "**Experiments:**\n",
        "We have 8 experiments with combinations of:\n",
        "\n",
        "LR ∈ {0.1, 0.01}, hidden ∈ {2,4}, bypass ∈ {False,True}\n",
        "\n",
        "This yields 2x2x2=8 experiments.\n",
        "\n",
        "Plus a 9th custom experiment: LR=1, hidden=1, bypass=True.\n"
      ],
      "metadata": {
        "id": "TgfGPZXJ3ojU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import statistics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_x = torch.tensor([[0., 0.],\n",
        "                        [0., 1.],\n",
        "                        [1., 0.],\n",
        "                        [1., 1.]], dtype=torch.float32)\n",
        "train_y = torch.tensor([[0.],\n",
        "                        [1.],\n",
        "                        [1.],\n",
        "                        [0.]], dtype=torch.float32)\n",
        "val_x = torch.tensor([[0.,0.],\n",
        "                      [0.,1.],\n",
        "                      [1.,0.],\n",
        "                      [1.,1.],\n",
        "                      [1.,0.1],\n",
        "                      [1.,0.9],\n",
        "                      [0.9,0.9],\n",
        "                      [0.1,0.9]], dtype=torch.float32)\n",
        "val_y = torch.tensor([[0.],\n",
        "                      [1.],\n",
        "                      [1.],\n",
        "                      [0.],\n",
        "                      [1.],\n",
        "                      [0.],\n",
        "                      [0.],\n",
        "                      [1.]], dtype=torch.float32)\n",
        "\n",
        "MAX_EPOCHS = 40000\n",
        "PATIENCE = 10\n",
        "IMPROVEMENT_THRESHOLD = 0.0001\n",
        "VAL_LOSS_GOAL = 0.2\n",
        "experiment_params = [\n",
        "    (0.1, 2, False),\n",
        "    (0.1, 2, True),\n",
        "    (0.1, 4, False),\n",
        "    (0.1, 4, True),\n",
        "    (0.01, 2, False),\n",
        "    (0.01, 2, True),\n",
        "    (0.01, 4, False),\n",
        "    (0.01, 4, True),\n",
        "    (1.0, 1, True)\n",
        "]"
      ],
      "metadata": {
        "id": "a0-MkpaU-kc_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Definition**"
      ],
      "metadata": {
        "id": "xu4rpu44AYou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple Multi-Layer Perceptron (MLP) for XOR-like tasks.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    hidden : nn.Linear\n",
        "        The hidden layer mapping from input (2D) to a hidden dimension.\n",
        "    output : nn.Linear\n",
        "        The output layer mapping from hidden representation (and possibly inputs if bypass is True) to a single scalar output.\n",
        "    bypass : bool\n",
        "        If True, the original inputs are concatenated to the hidden layer output before the output layer.\n",
        "    activation : nn.Module\n",
        "        The non-linear activation function used in the hidden layer and output.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    hidden_size : int\n",
        "        The number of hidden neurons.\n",
        "    bypass : bool, optional\n",
        "        Whether to concatenate the input directly to the output layer's input. Default is False.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, bypass=False):\n",
        "        super(Network, self).__init__()\n",
        "        self.bypass = bypass\n",
        "        self.hidden = nn.Linear(2, hidden_size)\n",
        "        out_input_size = hidden_size + (2 if bypass else 0)\n",
        "        self.output = nn.Linear(out_input_size, 1)\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the network.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor of shape (N, 2).\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        torch.Tensor\n",
        "            The scalar output after the forward pass, shape (N, 1).\n",
        "        \"\"\"\n",
        "        h = self.activation(self.hidden(x))\n",
        "        if self.bypass:\n",
        "            h = torch.cat((x, h), dim=1)\n",
        "        return self.activation(self.output(h))\n",
        "\n",
        "\n",
        "def train_model(lr, hidden, bypass):\n",
        "    \"\"\"\n",
        "    Train a model with given parameters until one of the stop conditions is met.\n",
        "\n",
        "    Stopping Conditions:\n",
        "    - Success: If validation loss hasn't improved by more than 0.0001 in the last\n",
        "      10 epochs and the best validation loss so far is < 0.2.\n",
        "    - Failure: If we reach 40,000 epochs without meeting the success condition.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    lr : float\n",
        "        Learning rate for the optimizer.\n",
        "    hidden : int\n",
        "        Number of hidden units.\n",
        "    bypass : bool\n",
        "        Whether to use a bypass connection (concatenating inputs to the output layer).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    success : bool\n",
        "        True if the training stopped successfully, False if failed.\n",
        "    epoch : int\n",
        "        Number of epochs trained.\n",
        "    train_loss_val : float\n",
        "        Final train loss value at stopping.\n",
        "    val_loss_val : float\n",
        "        Final validation loss value at stopping.\n",
        "    model : Network\n",
        "        The trained model instance.\n",
        "    \"\"\"\n",
        "    model = Network(hidden, bypass)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    epoch = 0\n",
        "    success = False\n",
        "\n",
        "    while epoch < MAX_EPOCHS:\n",
        "        epoch += 1\n",
        "        y_pred = model(train_x)\n",
        "        train_loss = loss_fn(y_pred, train_y)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            val_pred = model(val_x)\n",
        "            val_loss = loss_fn(val_pred, val_y)\n",
        "        if val_loss < best_val_loss - IMPROVEMENT_THRESHOLD:\n",
        "            best_val_loss = val_loss.item()\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "        if epochs_no_improve >= PATIENCE and best_val_loss < VAL_LOSS_GOAL:\n",
        "            success = True\n",
        "            break\n",
        "        if epoch >= MAX_EPOCHS and not success:\n",
        "            break\n",
        "    return success, epoch, train_loss.item(), val_loss.item(), model"
      ],
      "metadata": {
        "id": "AU0P7ljr-miS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Running All Experiments**\n",
        "note 1 - If there's more than one sample, we can compute standard deviation\n",
        "note 2 - STD% is defined as (std / mean) * 100\n"
      ],
      "metadata": {
        "id": "kcI4FUq6AkJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_results = []\n",
        "\n",
        "for i, (lr, hidden, bypass) in enumerate(experiment_params):\n",
        "    print(f\"=== Experiment {i+1} / 9 ===\")\n",
        "    print(f\"Params: LR={lr}, Hidden={hidden}, Bypass={bypass}\")\n",
        "    successes = 0\n",
        "    fail_count = 0\n",
        "    epochs_list = []\n",
        "    train_losses_list = []\n",
        "    val_losses_list = []\n",
        "    models_list = []\n",
        "    while successes < 10:\n",
        "        success, epochs, train_l, val_l, model = train_model(lr, hidden, bypass)\n",
        "        if success:\n",
        "            successes += 1\n",
        "            epochs_list.append(epochs)\n",
        "            train_losses_list.append(train_l)\n",
        "            val_losses_list.append(val_l)\n",
        "            models_list.append(model)\n",
        "        else:\n",
        "            fail_count += 1\n",
        "    mean_epochs = statistics.mean(epochs_list)\n",
        "    std_epochs = (statistics.pstdev(epochs_list)*100/mean_epochs) if len(epochs_list)>1 else 0\n",
        "    mean_train_loss = statistics.mean(train_losses_list)\n",
        "    std_train_loss = statistics.pstdev(train_losses_list) if len(train_losses_list)>1 else 0\n",
        "    mean_val_loss = statistics.mean(val_losses_list)\n",
        "    std_val_loss = statistics.pstdev(val_losses_list) if len(val_losses_list)>1 else 0\n",
        "\n",
        "    experiment_results.append({\n",
        "        'lr': lr,\n",
        "        'hidden': hidden,\n",
        "        'bypass': bypass,\n",
        "        'mean_epochs': mean_epochs,\n",
        "        'std_epochs_%': std_epochs,\n",
        "        'mean_train_loss': mean_train_loss,\n",
        "        'std_train_loss': std_train_loss,\n",
        "        'mean_val_loss': mean_val_loss,\n",
        "        'std_val_loss': std_val_loss,\n",
        "        'fail_count': fail_count,\n",
        "        'models': models_list\n",
        "    })\n",
        "    print(\"Results:\")\n",
        "    print(f\"Mean epochs: {mean_epochs:.2f} (std %: {std_epochs:.2f}%)\")\n",
        "    print(f\"Mean Train Loss: {mean_train_loss:.4f} (std: {std_train_loss:.4f})\")\n",
        "    print(f\"Mean Val Loss: {mean_val_loss:.4f} (std: {std_val_loss:.4f})\")\n",
        "    print(f\"Failed runs until 10 successes: {fail_count}\")\n",
        "    print(\"==========================================\\n\")"
      ],
      "metadata": {
        "id": "D0Kcho20-qad",
        "outputId": "bd23c6f0-308b-4178-da40-7fd72fd80d82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Experiment 1 / 9 ===\n",
            "Params: LR=0.1, Hidden=2, Bypass=False\n",
            "Results:\n",
            "Mean epochs: 12857.70 (std %: 32.79%)\n",
            "Mean Train Loss: 0.0295 (std: 0.0037)\n",
            "Mean Val Loss: 0.0390 (std: 0.0032)\n",
            "Failed runs until 10 successes: 0\n",
            "==========================================\n",
            "\n",
            "=== Experiment 2 / 9 ===\n",
            "Params: LR=0.1, Hidden=2, Bypass=True\n",
            "Results:\n",
            "Mean epochs: 12908.30 (std %: 42.30%)\n",
            "Mean Train Loss: 0.0390 (std: 0.0071)\n",
            "Mean Val Loss: 0.0564 (std: 0.0119)\n",
            "Failed runs until 10 successes: 0\n",
            "==========================================\n",
            "\n",
            "=== Experiment 3 / 9 ===\n",
            "Params: LR=0.1, Hidden=4, Bypass=False\n",
            "Results:\n",
            "Mean epochs: 9157.00 (std %: 11.61%)\n",
            "Mean Train Loss: 0.0266 (std: 0.0023)\n",
            "Mean Val Loss: 0.0377 (std: 0.0031)\n",
            "Failed runs until 10 successes: 0\n",
            "==========================================\n",
            "\n",
            "=== Experiment 4 / 9 ===\n",
            "Params: LR=0.1, Hidden=4, Bypass=True\n",
            "Results:\n",
            "Mean epochs: 10562.60 (std %: 16.32%)\n",
            "Mean Train Loss: 0.0330 (std: 0.0033)\n",
            "Mean Val Loss: 0.0494 (std: 0.0092)\n",
            "Failed runs until 10 successes: 0\n",
            "==========================================\n",
            "\n",
            "=== Experiment 5 / 9 ===\n",
            "Params: LR=0.01, Hidden=2, Bypass=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Detailed Analysis of the 9th Experiment**\n",
        "\n",
        "The 9th experiment parameters: (LR=1, hidden=1, bypass=True)\n"
      ],
      "metadata": {
        "id": "TgId9iHMBFG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp9 = experiment_results[8]\n",
        "model9 = exp9['models'][0]\n",
        "model9.eval()\n",
        "print(\"=== Experiment 9 Detailed Analysis ===\")\n",
        "print(\"Hidden neuron output on training set (A,B):\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    h_out = model9.activation(model9.hidden(train_x))\n",
        "print(\"Input (A,B) | Hidden Output | Target\")\n",
        "print(\"------------------------------------\")\n",
        "for i in range(len(train_x)):\n",
        "    inp = train_x[i].tolist()\n",
        "    hidden_output = h_out[i].item()\n",
        "    target = train_y[i].item()\n",
        "    print(f\"{inp}     {hidden_output:.4f}       {target}\")\n",
        "\n",
        "print(\"\\nAnalyze this output. Depending on initialization and training, the single hidden neuron\\n\"\n",
        "      \"may be acting as a threshold function distinguishing certain input regions. Try multiple runs\\n\"\n",
        "      \"to see if it behaves similarly or differently. See if it resembles a known logical function like\\n\"\n",
        "      \"AND, OR, or acts as a line separator enabling XOR behavior.\")\n"
      ],
      "metadata": {
        "id": "ufiS3G6V-yDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting Analysis**\n",
        "\n",
        "We can plot some results to understand the influence of hidden units, bypass, and learning rates."
      ],
      "metadata": {
        "id": "iB8Wnb8-BV0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp_data = experiment_results[:-1]\n",
        "hidden2 = [res for res in exp_data if res['hidden']==2]\n",
        "hidden4 = [res for res in exp_data if res['hidden']==4]\n",
        "mean_epochs_h2 = np.mean([r['mean_epochs'] for r in hidden2])\n",
        "mean_epochs_h4 = np.mean([r['mean_epochs'] for r in hidden4])\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(['Hidden=2','Hidden=4'], [mean_epochs_h2, mean_epochs_h4])\n",
        "plt.title('Mean Epochs Until Stopping by Number of Hidden Units')\n",
        "plt.ylabel('Mean Epochs')\n",
        "plt.show()\n",
        "\n",
        "bypass_true = [res for res in exp_data if res['bypass']==True]\n",
        "bypass_false = [res for res in exp_data if res['bypass']==False]\n",
        "\n",
        "mean_epochs_btrue = np.mean([r['mean_epochs'] for r in bypass_true])\n",
        "mean_epochs_bfalse = np.mean([r['mean_epochs'] for r in bypass_false])\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(['Bypass=True','Bypass=False'], [mean_epochs_btrue, mean_epochs_bfalse])\n",
        "plt.title('Mean Epochs Until Stopping by Bypass')\n",
        "plt.ylabel('Mean Epochs')\n",
        "plt.show()\n",
        "\n",
        "lr_01 = [res for res in exp_data if res['lr']==0.1]\n",
        "lr_001 = [res for res in exp_data if res['lr']==0.01]\n",
        "\n",
        "std_epochs_01 = np.mean([r['std_epochs_%'] for r in lr_01]) if len(lr_01)>0 else 0\n",
        "std_epochs_001 = np.mean([r['std_epochs_%'] for r in lr_001]) if len(lr_001)>0 else 0\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(['LR=0.1','LR=0.01'], [std_epochs_01, std_epochs_001])\n",
        "plt.title('Average STD% of Epochs by Learning Rate')\n",
        "plt.ylabel('STD% of Epochs')\n",
        "plt.show()\n",
        "\n",
        "print(\"All done!\")"
      ],
      "metadata": {
        "id": "ood8hyDW-7Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 2 - just for fun**"
      ],
      "metadata": {
        "id": "Icuz42Ev-c2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define XOR data with Gaussian noise\n",
        "def generate_xor_data(noise_std=0):\n",
        "    \"\"\"Generates XOR data with optional Gaussian noise.\"\"\"\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([0, 1, 1, 0])  # XOR outputs\n",
        "    if noise_std > 0:\n",
        "        X = X + np.random.normal(0, noise_std, X.shape)\n",
        "    return X, y\n",
        "\n",
        "# Define function to build a model with flexible architecture and activation\n",
        "def build_model(hidden_layers=1, neurons_per_layer=4, activation='relu', optimizer='adam', dropout_rate=0):\n",
        "    \"\"\"Builds a customizable MLP model for XOR learning.\"\"\"\n",
        "    model = Sequential()\n",
        "    # Input layer and first hidden layer\n",
        "    model.add(Dense(neurons_per_layer, input_dim=2, activation=activation))\n",
        "    if dropout_rate > 0:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    # Additional hidden layers\n",
        "    for _ in range(hidden_layers - 1):\n",
        "        model.add(Dense(neurons_per_layer, activation=activation))\n",
        "        if dropout_rate > 0:\n",
        "            model.add(Dropout(dropout_rate))\n",
        "    # Output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define function to visualize decision boundary\n",
        "def plot_decision_boundary(X, y, model):\n",
        "    \"\"\"Plots decision boundary for a trained model.\"\"\"\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                         np.arange(y_min, y_max, 0.01))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = (Z > 0.5).astype(int).reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
        "    plt.title(\"Decision Boundary for XOR\")\n",
        "    plt.show()\n",
        "\n",
        "# Main function to run experiments\n",
        "def run_experiments():\n",
        "    \"\"\"Runs advanced XOR learning experiments.\"\"\"\n",
        "    X, y = generate_xor_data(noise_std=0.1)  # Generate XOR data with noise\n",
        "\n",
        "    # Experiment parameters\n",
        "    experiments = [\n",
        "        {'hidden_layers': 1, 'neurons_per_layer': 4, 'activation': 'relu', 'optimizer': 'adam', 'dropout_rate': 0},\n",
        "        {'hidden_layers': 2, 'neurons_per_layer': 6, 'activation': 'relu', 'optimizer': 'adam', 'dropout_rate': 0.2},\n",
        "        {'hidden_layers': 1, 'neurons_per_layer': 4, 'activation': 'tanh', 'optimizer': 'rmsprop', 'dropout_rate': 0},\n",
        "        {'hidden_layers': 2, 'neurons_per_layer': 8, 'activation': 'elu', 'optimizer': 'sgd', 'dropout_rate': 0.3}\n",
        "    ]\n",
        "\n",
        "    for i, params in enumerate(experiments):\n",
        "        print(f\"\\nRunning Experiment {i + 1} with params: {params}\")\n",
        "        model = build_model(\n",
        "            hidden_layers=params['hidden_layers'],\n",
        "            neurons_per_layer=params['neurons_per_layer'],\n",
        "            activation=params['activation'],\n",
        "            optimizer=params['optimizer'],\n",
        "            dropout_rate=params['dropout_rate']\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(X, y, epochs=500, verbose=0, batch_size=4)\n",
        "\n",
        "        # Evaluate the model\n",
        "        loss, accuracy = model.evaluate(X, y, verbose=0)\n",
        "        print(f\"Experiment {i + 1} Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "        # Visualize decision boundary\n",
        "        plot_decision_boundary(X, y, model)\n",
        "\n",
        "# Run the experiments\n",
        "run_experiments()"
      ],
      "metadata": {
        "id": "peS0k8pUnhoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}